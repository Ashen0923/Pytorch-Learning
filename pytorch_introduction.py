# -*- coding: utf-8 -*-
"""Pytorch_Introduction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rFjJz2J-qjUcVz6k6ZuI0wn_zxfASiT0
"""

import torch
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
print(torch.__version__)

"""##Introduction to sensors

###Creating tensors

"""

#scalar
scalar = torch.tensor(7)
scalar

scalar.ndim

#Get tensor back as Python int
scalar.item()

#Vector
vector = torch.tensor([7,7])
vector

vector.ndim

vector.shape

#Matrix
MATRIX = torch.tensor([[7,8],
                       [9,10]])
MATRIX

MATRIX.ndim

MATRIX[1]

MATRIX.shape

#Tensor
TENSOR = torch.tensor([[[1,2,3],
                        [3,6,9],
                        [2,4,5]]])
TENSOR

TENSOR.ndim

TENSOR.shape

TENSOR1 = torch.tensor([[[1,2,3],
                        [3,6,9]]])
TENSOR1

TENSOR1.shape

"""###Random tensors

Why random tensors?
Random tensors are important because the way many neural networks learn is that theu start with tensors full of random numbes and t ehn adjust thise random numbers to better represent the data

`Start with random numners-> look at data ->update random numbers -> look at data -> update random numbers`
"""

#Create a d random tensor of size(3,4)
random_tensor = torch.rand(3,4)
random_tensor

random_tensor.ndim

#Create a random tensor with similar shape to an image tensor
random_image_size_tensor = torch.rand(size=(224,224,3)) #height, width, color channels(R,G,B)
random_image_size_tensor.shape, random_image_size_tensor.ndim

"""###Zeros and ones"""

#Create a tensor of all zeros
zeros = torch.zeros(size=(3,4))
zeros

#Create a tensor of all ones
ones = torch.ones(size=(3,4))
ones

ones.dtype

"""###Creating a range of tensors and tensors-like"""

#Use torch.arange()
torch.arange(1,11)

range1 = torch.arange(start=0, end=1000, step=77)
range1

#Creating tensors like
ten_zeros = torch.zeros_like(input=range1)
ten_zeros

"""###Tensor datatypes

**Note:** Tensor datatypes is one of the 3 big errors with PyTorch and Deep Learning:
1. Tensors not right datatype
2. Tensors not right shape
3. Tensors not on the right device
"""

#Float 32 tensor
float_32_tensor = torch.tensor([3.0,6.0,9.0],
                                dtype=None, #what datatype is the tensor (e.g. float32 or float16)
                                device=None, #what device is your tensor on
                               requires_grad=False) #whether or not to track gradients with this tensors operations
float_32_tensor

float_32_tensor.dtype

float_16_tensor = float_32_tensor.type(torch.float16)
float_16_tensor

float_16_tensor*float_32_tensor

int_32_tensor = torch.tensor([3,6,9], dtype=torch.int32)
int_32_tensor

float_32_tensor*int_32_tensor

"""###Getting information from tensors (tensor attributes)

1.Tensors not right datatype - to get datatype from a tensor, can use `tensor.dtype`

2.Tensors not right shape - to get shape from a tensor, can use `tensor.shape`

3.Tensors not on the right device - to get device from a tensor, can use `tensor.device`
"""

#Create a tensor
some_tensor = torch.rand(3,4)
some_tensor

#Find out details about some tensor
print(some_tensor)
print(f"Datatype of tensor: {some_tensor.dtype}")
print(f"Shape of tensor: {some_tensor.shape}")
print(f"Device tensor is on: {some_tensor.device}")

"""###Manipulating Tensors (tensor operations)

Tensor operations include:
* Addition
* Subtaction
* Multiplication ( element-wise)
* Division
* Matrix mulitplication
"""

#Create a tensor
tensor = torch.tensor([1, 2,3])
tensor + 10

#Multipl tensor by 10
tensor = tensor * 10
tensor

#Subtract 10
tensor - 10

#Try out Pytorch-inbulit functions
torch.mul(tensor, 10)

torch.add(tensor, 10)

"""###Matrix Multiplication

Two main ways of performing multiplication in neura netwoks and deep learning:

1. Element-wise multiplication
2. Matri multiplication (dot product)


1.There are two main rules that performing matrix mulitplication needs to satisfy:
1 The **inner dimensions** must match:
* `(3,2) @ (3,2)` won't work
* `(2,3) @ (3,2)` will work
* `(3,2) @ (2,3)` will work

2. The resulting matric has the shape of the **outer dimensions**:
* `(2,3) @ (3,2)` -> `(2,2)`
* `(3,2) @ (2,3)` -> `(3,3)`
"""

#Element wise mulitplicatiom
print(tensor, "*" , tensor)
print(f"Equals: {tensor * tensor}")

#Matrix Multiplication
torch.matmul(tensor,tensor)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# value = 0
# for i in range(len(tensor)):
#   value += tensor[i] * tensor[i]
# print(value)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# torch.matmul(tensor,tensor)

"""### One of the most common errors in deep learning is Shape Errors

"""

#Shapes for matrix multiplication
tensor_A = torch.tensor([[1,2],
                        [3,4],
                        [5,6]])

tensor_B = torch.tensor([[7,10],
                        [8,11],
                        [9,12]])

#torch.mm(tensor_A,tensor_B) #torch.mm is the same as torch.matmul
torch.matmul(tensor_A,tensor_B)

tensor_A.shape, tensor_B.shape

"""To fix our tensor shape issues, we can manipuate the shape of one of our tensors using a **transpose**.

A **transpose** switches the axes or dimensions of a given tensor.
"""

tensor_B.T, tensor_B.T.shape

#The matrix nultiplication operation works when tensor_B is transposed
print(f"Oriiginal shapes: tensor_A = {tensor_A.shape}, tensor_B = {tensor_B.shape}")
print(f"New shapes: tensor_A = {tensor_A.shape} (same shape as above), tensor_B.T = {tensor_B.T.shape}")
print(f"Multiplying: {tensor_A.shape} @ {tensor_B.T.shape} <- inner dimensions match")
print("Output:\n")
output = torch.matmul(tensor_A,tensor_B.T)
print(output)
print(f"\nOutput shape: {output.shape}")

"""##Finding the min,max,mean,sum,etc(tensor aggregation)"""

#Create a tensor
x = torch.arange(0,100,10)
x

#Find the min
torch.min(x), x.min()

#Find the max
torch.max(x) , x.max()

#Find the mean - note: the torch.mean() function requires a tensor of float32 datatype
torch.mean(x.type(torch.float32)),x.type(torch.float32).mean()

#Find the sum
torch.sum(x), x.sum()

"""##Finding the positional min and max"""

x

#Find the position in tensor that has the minimum value with argmin() - > returns index position of target tensor where the minimum value occurs
x.argmin()

#Find the position in tensor that has the maximum value with argmax()
x.argmax()



"""##Reshaping, stacking, squeezing and unsqueezing tensors

* Reshaping - reshape and input tensor to a defined shape
* View - Return a view of an input tenso of cetain shape but keep the same memory as the original tensor
* Stacking - combine multiple tensors on top of each other (vstack) or side by side(hstack)
* Squeeze - removes all 'l' dimensions from a tensor
* Unsqueeze - add a dimension to a target tensor
* Permute - Return a view of the input with dimensions permuted(swapped) in a certain way
"""

#lets create a tensor
import torch
x = torch.arange(1.,10.)
x,x.shape

#Add an extra dimension
x_reshaped = x.reshape(1,9)
x_reshaped, x_reshaped.shape

#Change the view
z = x.view(1,9)
z,z.shape

#changing a changes x (because a view o tensor shares the same memory a the original)
z[:,0] = 5
z,x

#stack tensors on top of each other
x_stacked = torch.stack([x,x,x,x], dim =0)
x_stacked

#torch.squeeze() - removes all single dimensions from a target tensor
print(f"Previous tensor: {x_reshaped}")
print(f"Previous shape: {x_reshaped.shape}")

#Remove extr dimensions from x_reshaped
x_squeezed = x_reshaped.squeeze()
print(f"\nNew tensor: {x_squeezed}")
print(f"New shape: {x_squeezed.shape}")
x_reshaped

x_reshaped.shape

x_reshaped.squeeze()

x_reshaped.squeeze().shape

#torch.unsqueeze() - adds a single dimension to a target tensor at a specefic dim(dimension)
print(f"Previous target: {x_squeezed}")
print(f"Previous shape: {x_squeezed.shape}")

#Add an extra dimension with unsqueeze
x_unsqueezed = x_squeezed.unsqueeze(dim=0)
print(f"\n New tensor: {x_unsqueezed}")
print(f"New shape: {x_unsqueezed.shape}")

#torch.permute - rearranges the dimensions of a target tensor in a specefic order
x_original = torch.rand(size=(224,224,3)) #[height,width,colour_channels]

#Permute the original tensor to rearrange the axis (or dim) order
x_permuted = x_original.permute(2,0,1) #shifts axis 0->1, 1->2, 2->0

print(f"Previous shape: {x_original.shape}")
print(f"New shape: {x_permuted.shape}")#[coloir_chnne;s,height,width]

x_original[0,0,0]

"""##Indecing (selectong data from tensors)

Indexing with Pytorch is similar to indexing with Numpy
"""

#Create a tensor
import torch
x = torch.arange(1,10).reshape(1,3,3)
x,x.shape

#lets index on our new tensor
x[0]

#lets index on our middle bracket
x[0][0]

#lets index on our inner bracket
x[0][0][1]

#You can also use ":" to select "all" of a target dimension
x[:,0]

#Get all values of 0th and 1st dimensions but only index 1 and 2nd dimension
x[:,:,1]

#Get all values of 0th dimension but only index value of 1st and 2nd dimension
x[:,1,1]

#Get index 0 of 0th and 1st dimension and all values of 2nd dimnesion
x[0,0,:]



#Index on x to return 9
x[0][2][2]

#Index on x to retun 3,6,9
x[:,:,2]

"""##PyTorch Tensors and Numpy

Numpy is a popular scientific Python numerical computing library

And because of this, PyTorch has functionality to interact with it

* Data in NumPy, wan in PyTorch Tensor -> `torch.from_numpy(ndarray)`
* PyTorch tensor -> NumPy -> `torch.Tensor.Numpy()
"""

#NumPy array to tensor
import torch
import numpy as np

array = np.arange(1.0,8.0)
tensor = torch.from_numpy(array) #warning: when coverting from numpy -> pytorch, pytorch reflects numpy's default datatype of float64 unless specified otherwise

array.dtype

tensor.dtype

#change the value of array, what will this do to 'tensor'?
array = array +1
array,tensor

#Tensor to Numpy array
tensor = torch.ones(7)
numpy_tensor = tensor.numpy()
tensor,numpy_tensor

#Change the tensor, what happened to  'numpy_tensor'?
tensor = tensor +1
tensor,numpy_tensor

"""##Reproducibility (trying to take radom out of random)

In short how a neural network learns:

`start with random numbers -> tensor operations -> update random numbers to try and make them better representations of the data -> again ->again -> again....`

To reduce the randomness in neural networks and PyTorch comes the concept of a **random seed**.

Essentually what the random seed does is "Flavour" the randomness
"""

import torch

#Create two random tensors
random_tensor_A = torch.rand(3,4)
random_tensor_B = torch.rand(3,4)

print(random_tensor_A)
print(random_tensor_B)
print(random_tensor_A == random_tensor_B)

#Lets make some random but reprodcable tensors
import torch

#Set the random seed
RANDOM_SEED = 42
torch.manual_seed(RANDOM_SEED)
random_tensor_C = torch.rand(3,4)

torch.manual_seed(RANDOM_SEED)
random_tensor_D = torch.rand(3,4)

print(random_tensor_C)
print(random_tensor_D)
print(random_tensor_C == random_tensor_D)



"""##Running Tensors and PyTorch objects on the GPUs (and making faster computations)

GPU = faster computation on numbers , thanks to cuda + NVIDIA hardware + PyTorch working behind the scenes to mae everything hunky dory (good)

###Getting a GPU

1.Easiest - Use Google Colab for a free GPU ( options to upgrade as well)
2. Use your own GPU - takes a little bit of setup and requires the investment of purchasing a GPU, there's lots of options... see this
3. Use cloud computing - GCP, AWS, Azure, these services allow you to rent computers on the cloud and access them

For 2,3 PyTorch + GPU drivers (CUDA) takes a little bit of setting up, to do this, refer to PyTorch setup documentation
"""

!nvidia-smi

"""### 2 Check for GPU access with PyTorch"""

# Check for GPU access with PyTorch

import torch
torch.cuda.is_available()

#Setup device agnostic cide
device = "cuda" if torch.cuda.is_available() else "cpu"
device

#Count number of devices
torch.cuda.device_count()

"""## 3 Putting tensors (and models) on the GPU

The reason we want our tensors/models on the GPU is beacause using a GPU results in faster computations



"""

#Create a tensor (default on the CPU)
tensor = torch.tensor([1,2,3], device ="cpu")

#Tensor not on GPU
print(tensor, tensor.device)

#Move tensor to GPU (if available)
tensor_on_gpu = tensor.to(device)
tensor_on_gpu

"""### 4. Movig tensors back to the CPU"""

tensor_on_gpu.numpy()

#To fix the GPU tnesor with Numpy issure, we can fist set it to  the CPU
tensor_back_on_xcpu = tensor_on_gpu.cpu().numpy()
tensor_back_on_xcpu



"""## Exercises & Extra-curicullum"""

