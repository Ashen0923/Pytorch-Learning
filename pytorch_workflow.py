# -*- coding: utf-8 -*-
"""PyTorch WorkFlow.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PTu8VFWYgCSOudI8AWhi82SN-KPSqUyy

##PyTorch WOrkflow

Lets explore an example PyTorch end-to-end workflow
"""

what_were_covering = {1: "Data(prepare and load)",
                      2: "Build Model",
                      3: "Fitting the model to data(Training)",
                      4: "Making Predictions and evaluating a model(inference)",
                      5: "Saving and loading a model",
                      6: "Putting it all together"}

what_were_covering

import torch
from torch import nn #nn contains all of Pytorch's building blocks for neural networks
import matplotlib.pyplot as plt

#Check PyTorch version
torch.__version__

"""##Data Preparing and Loading

Data can be almost anything... in machine learning.

* Excel Spreadsheet
* Images of any Kind
* Videos (Youtube has lots of data..)
* Audio like songs or podcsts
* DNA
* Text

Machine learning is a game of two parts:
  1. Get data into a numerical representatiom
  2. Build a model to learn patterns in that numerical representation

  To showcase this lets create some *known* data using the linear regression formula

  We'll use a linear regrssion formula to make a straight line with known **parameters**
"""

import torch

#Create *known* parameters
weight = 0.7
bias = 0.3

#Create
start =0
end = 1
step = 0.02

X = torch.arange(start, end, step).unsqueeze(dim=1)
y = weight * X + bias

X[:10], y[:10], len(X),len(y)



"""###Splitting data in trainig and test sets.(one of the most important concepts in machine learning in general)

lets create a training and test set with our data

"""

#Create a train/test split
train_split = int(0.8 * len(X)) #80% of data used for training set, 20% for testing
X_train, y_train = X[:train_split], y[:train_split]
X_test, y_test = X[train_split:], y[train_split:]

len(X_train), len(y_train), len(X_test), len(y_test)

"""How might we better visualize our data?

This where the data explorers's motto comes in

"Visualize,visualize,visualize!"
"""

def plot_predictions(train_data= X_train,
                     train_labels = y_train,
                     test_data = X_test,
                     test_labels=y_test,
                     predictions=None):



  #Plots training data, test data and compare predictions
  plt.figure(figsize=(10,7))

  #Plot training data in blue
  plt.scatter(train_data, train_labels, c="b", s=4, label="Training Data")

  #Plot test data in green
  plt.scatter(test_data, test_labels, c="g", s=4, label="Test Data")

  #Are there predictions?
  if predictions is not None:
    #Plot the preditions if they exist
    plt.scatter(test_data, predictions, c="r", s=4, label="Predictions")

  #Show the legend
  plt.legend(prop={"size":14});

plot_predictions();

"""##2. Build Model

Our first PyTorch model!
This is very exciting... let's do it

what our model does
* Starting with random values(weight & bias)
* Look at traning dta and adjust the random values to better represent (or get closer) to ideal values (the weight and bias we used to create the data

How does ot do so?

Through two main algorithms
1. Gradient descent
2. Backpropagation
"""

from torch import nn

# Create Linear regression model class
class LinearRegressionModel(nn.Module): # <- almost everything in PyTorch
  def __init__(self):
    super().__init__()
    self.weights = nn.Parameter(torch.randn(1,requires_grad = True, dtype=torch.float))
    self.bias = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))

  #Forward method to define the computation in the model
  def forward(self, x: torch.Tensor) -> torch.Tensor:
    return self.weights * x + self.bias

"""###PyTorch Model Building Essentials

* torch.nn - contains all of the buildings for computational graphs ( a neural netwirk cn be considered a computational graph)
* torch.nn.parameter - what parameters should our model try and learn, often a pytorch layer from torch.nn will set them for us
* torch.nn.module - base class for all neural network modules,if you subclass it,you should override forward()
* torch.optim - this where the optimizers in Pytorch live, they will help gradient descent.
* def forward() - All nn.module subclasses requie you to overwrite forward, this method defines what happens in the forward computation

### Checkig the contents of our Pytorch model

Now we've created a model, lets see what inside...

SO we can check our model parametersor what inside our model using `.parmeters().`
"""

#Create a random seed.
torch.manual_seed(42)

#Create an instance of the model (this is a subclass of nn.Module)
model_O = LinearRegressionModel()

#Check out the parameters
list(model_O.parameters())

#List naemd parameters
model_O.state_dict()

weight,bias

"""### Making prediction using `torch.inference_mode)_`

To check our model's predictive power, let's see how well it predicts `y_test` based on `X_test`

When we pass data through our model, it's going to run it through the `forward()` method.
"""

#Make predictions with model
with torch.inference_mode():
  y_preds = model_O(X_test)

y_preds

#y_preds = model_O(X_test)
#y_preds

y_test

plot_predictions(predictions=y_preds)

"""###3 Train Model

The whole idea of training is for a model to a move from some *unkownn* parameters to sime *known* paramaters

One way to measure how poor or how wrong your models pedicitions are is to use a loss function.

* Note: Loss function may also be called cost function or criterion in different areas. For our case, we're going to refer to it as a loss function

Things we need to train

**Loss function:** A function to measure how wrong your model's predictions are to the ideal outputs, lower is better.

**Optimizer**  Takes into account the loss of a model and adjust the model's parameters (eg: weight & bias in our case) to improve the loss function

And specifically for pytorch we need a
* training loop
* testing loop
"""

list(model_O.parameters())

#Check out our model's parameters (a parameter is a value that the model sets itself)
model_O.state_dict()

#Setup a loss function
loss_fn = nn.L1Loss()

#Setup an optimizer(stochastic gradient descent)
optimizer = torch.optim.SGD(params=model_O.parameters(),
                            lr=0.01) #learning rate = possibly the most important hyperparameter you can set

"""**Q**: Which loss function and optimizer should I use?

**A**: This will be problem specefic But with experience,ypu'll get and idea of what works and what doesn't with your particula problem set

For example, for a regression problem, a loss function of `nn.L1Loss()` and optimizer like `torch.optim.SGD()` will suffice

But for a classification problem like classifying where a photo is of a dog or cat, you'll liely want to use a loss function of `nn.SCELoss()` (Binary Cross Entropy Loss)

### Building a Training Loop(and a testing loop) in PyTorch

A couple of things we need in a traning loop:

0. Loop through the data
1. Forward pass (this involves data moving through our model's `forward()` functions) - also called foward propagation
2. Calculate the loss (compare forward pass predictions to ground truth labels)
3. Optimizer zero grad
4. Loss backward - move backwards through the network to calculate the grradients of each of the parameters of ore model with respect to the loss (**backpropgation**)
5. Optimizer step - use the optimizer to adjust our model's performance to try and improve the loss(**gradient descent**)
"""

torch.manual_seed(42)

#An epoch is one loop through the data
epochs = 200

#Track different values
epoch_count = []
loss_values = []
test_loss_values = []

# 0. loop through the data
for epoch in range(epochs):
  #Set the model to traning mode
  model_O.train() #train mode in PyTorcg sets all parameters that requires gradients to require graduents

  # 1. Forward pass
  y_pred = model_O(X_train)

  # 2. Calculate the loss
  loss = loss_fn(y_pred, y_train)
  #print(f"Loss: {loss}")

  # 3. Optimizer zero grad
  optimizer.zero_grad()

  # 4. Loss backward (Perform backpropagation)
  loss.backward()

  # 5. Optimizer step (perform gradient descent)
  optimizer.step()#By default how the optimizer changes will accumulate through the loop... So we have to zero them above in step 3  for the next iteration of the loop


  ###Testing
  model_O.eval() #turns off differenet settings in the model not noeded for evaluation/testing
  with torch.inference_mode(): #turns off gradient tracking and  a couple more things behind the scenes
    test_pred = model_O(X_test) #do the forward pass
    test_loss = loss_fn(test_pred, y_test) #calculate the loss
  #print(f"Test loss: {test_loss}")
  if epoch % 10 ==0:
    epoch_count.append(epoch)
    loss_values.append(loss)
    test_loss_values.append(test_loss)
    print(f"Epoch: {epoch} | Loss: {loss} | Test Loss: {test_loss}")
    print(model_O.state_dict())#Print out model state_dict

model_O.state_dict()

weight,bias

with torch.inference_mode():
  y_preds_new = model_O(X_test)

y_preds_new

plot_predictions(predictions=y_preds_new)

epoch_count, loss_values, test_loss_values

import numpy as np
np.array(torch.tensor(loss_values).numpy())

#Plot the loss curves
plt.plot(epoch_count, np.array(torch.tensor(loss_values).numpy()), label="Train loss")
plt.plot(epoch_count, test_loss_values, label="Test loss")
plt.title("Training and test loss curves")
plt.ylabel("Loss")
plt.xlabel("Epochs")
plt.legend()



"""### Saving a model in PyTorch

There are three main methods you should about for saving and loading models in Pytorch

1 `torch.save()` - allows you save a PyTorch object in Python's pickle format

2. `torch.load()` - allows you load a saved PyTorch Project

3. `torch.nn.Module.load_state_dict() - this allows to load a model's saved state dictionary




"""

model_O.state_dict()

#Saving our PyTorch Model
from pathlib import Path

#1. Create models directory
MODEL_PATH = Path("models")
MODEL_PATH.mkdir(parents=True, exist_ok=True)

#2. Create model save path
MODEL_NAME = "01_pytorch_workflow_model_0.pth"
MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME

#3. Save the model state dict
print(f"Saving model to: {MODEL_SAVE_PATH}")
torch.save(obj=model_O.state_dict(), f=MODEL_SAVE_PATH)

!ls -l models

"""##Loading a PyTorch Model

Since we saved our model's `state_dict()` rathehr the entire model, we'll create a new instance of our model class and load the saved `state_dict()` into that

"""

model_O.state_dict()

# To load in a saved state_dict we have to instantiate a new instance of our model class
loaded_model_O = LinearRegressionModel()

#Load the saved state_dict of model_O (this will update the new instance with updated parameters)
loaded_model_O.load_state_dict(torch.load(f=MODEL_SAVE_PATH))

loaded_model_O.state_dict()

#Make some predictions with our loaded model
loaded_model_O.eval()
with torch.inference_mode():
  loaded_model_preds = loaded_model_O(X_test)

loaded_model_preds

y_preds_new == loaded_model_preds

"""##6. Putting it all together

Let's go back through the steos above and see it all in one place
"""

#Import PyTorch and matplotlib
import torch
from torch import nn
import matplotlib.pyplot as plt

#Check PyTorch version
torch.__version__

"""Create device-agnostic code.

This meands if we've got access to a GPU, our code will use it (for potentially faster computing).

If no GPU is available, the code will default to using CPU.


"""

#setup device agnostic code
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

!nvidia-smi

"""###6.1 Data"""

#Create some data using the linear regression formula of y = weight * X + bias
weight = 0.7
bias = 0.3

#Create range values
start = 0
end = 1
step = 0.02

#Create x and y (features and labels)
X = torch.arange(start, end, step).unsqueeze(dim=1)
y = weight * X + bias

X[:10], y[:10]

#Split the data
train_split = int(0.8 * len(X))
x_train, y_train = X[:train_split], y[:train_split]
x_test, y_test = X[train_split:], y[train_split:]

len(x_train), len(y_train), len(x_test), len(y_test)

#Plot the data
#Note:if you don't have the plot predictions() function loaded, this will error
plot_predictions(x_train, y_train, x_test, y_test)

"""##6.2 Building a Pytorch linear model"""

#Create a linear model by subclassing nn.Module
class LinearRegressionModelV2(nn.Module):
  def __init__(self):
    super().__init__()
    #Use nn.Linear()for creating the model parameters / also called: linear transform, probing layer, fully connected layer, dense layer
    self.linear_layer = nn.Linear(in_features = 1,out_features=1)

  def forward(self,x: torch.Tensor) -> torch.Tensor:
    return self.linear_layer(x)

#Set the manual seed
torch.manual_seed(42)

#Create an instance of the model
model_1 = LinearRegressionModelV2()
model_1,model_1.state_dict()

"""* `nn.linear` inbuilt function  gives the fomula in the fowarwad function. So there is no need to write the formula in the forward funtion when using the inbuilt class.

* These linear functions are used in layer structural neural network such as FC, dense and other layers.
"""

#check the model current device
next(model_1.parameters()).device

#Set the model to use the target device
model_1.to(device)
next(model_1.parameters()).device

"""### 6.3 Training

For training we need:
* Loss Function
* Optimizer
* Training loop
* Testing loop
"""

#Setup loss function
loss_fn == nn.L1Loss()

#Setup our optimizer
optimizer = torch.optim.SGD(params=model_1.parameters(),lr=0.01)

#let's write a training loop
torch.manual_seed(42)

epochs = 200

#Put data on the target device
x_train = x_train.to(device)
y_train = y_train.to(device)
x_test = x_test.to(device)
y_test = y_test.to(device)

for epoch in range(epochs):
  model_1.train()

  #1. Forward pass
  y_pred = model_1(x_train)

  #2. Calculate the loss
  loss = loss_fn(y_pred, y_train)

  #3. Optimizer zero grad
  optimizer.zero_grad()

  #4. Perform backpropagation
  loss.backward()

  #5. Optimizer step
  optimizer.step()

  ###Testing
  model_1.eval()
  with torch.inference_mode():
    test_pred = model_1(x_test)
    test_loss = loss_fn(test_pred, y_test)

  #Print out whats happening

  if epochs % 10 == 0:
    print(f"Epoch: {epoch} | Loss: {loss} | Test Loss: {test_loss}")

model_1.state_dict()

weight,bias

"""###6.4 Making and evaluating predictions"""

# Turn model into evaluation mode
model_1.eval()

# Turn on inference context manager
with torch.inference_mode():
  # Make predictions on the test data
  y_preds = model_1(x_test)

y_preds

#Check out our model predictions visually
plot_predictions(predictions=y_preds.cpu())

"""###6.5 Saving & Loading a trained model"""

from pathlib import Path

#1. Create models directory
MODEL_PATH = Path("models")
MODEL_PATH.mkdir(parents=True, exist_ok=True)

#2. Create a model save path
MODEL_NAME = "01_pytorch_workflow_model_1.pth"
MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME

#3. Save the model state dict
print(f"Saving model to: {MODEL_SAVE_PATH}")
torch.save(obj=model_1.state_dict(), f=MODEL_SAVE_PATH)

model_1.state_dict()

#load a PyTorch

#Create a new instance of linear regression model V2
loaded_model_1 = LinearRegressionModelV2()

#load the saved model_1 state_dict
loaded_model_1.load_state_dict(torch.load(f=MODEL_SAVE_PATH))

#Put the loaded model to device
loaded_model_1.to(device)

next(loaded_model_1.parameters()).device

loaded_model_1.state_dict()

#Evaluate loaded model
loaded_model_1.eval()
with torch.inference_mode():
  loaded_model_preds = loaded_model_1(x_test)

loaded_model_preds ==y_preds

